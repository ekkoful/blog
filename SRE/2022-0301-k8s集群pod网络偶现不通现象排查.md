# Kubernete集群Pod网络偶现不通现象排查
## 问题背景

关于这个问题的背景是，我们使用的是阿里云ACK的托管k8s集群，并且使用的阿里云中的Terway网络，作为k8s中CNI虚拟网络组件，某一天发现集群中的有一个node的节点上的部分虚拟网段的pod无法和外部进行通信。（集群虚拟网络和集群外网络都无法进行通信）

## 排查思路
### 虚拟网络层排查
集群网络中出现问题，首先可能会想到虚拟网络层通信出现了问题，这种问题可能是以
下情况:
- 网络组件和etcd通信出现了问题，因为很多类型的虚拟网络，
- etcd出现了问题，但是一般etcd出现问题，可能会导致整个k8s集群都出现问题
### 主机层排查
因为只是这个Node节点上的pod无法进行通信，所以猜测是否主机网络出现了问题，主机可能存在以下问题
- 主机脱离集群，这时在`kubectl get node -o wide`中可以看到主机的status
- 主机路由出现问题
### 排查工具
- `tcpdump`命令进行抓包分析，这个时候抓包要明确具体抓取哪个网卡的流量
### aliyun terway网络工作模式的理解
- vpc模式
- ENI模式
- ENI多IP模式
    - veth策略路由模式
    - ipvlan l2模式
## 解决方案
通过排查，以及对应的抓包分析和对于terway网络模式的研究，发现是主机虚拟网卡丢失了，所以需要重新进行挂载虚拟网卡，对此我们暂时不清楚aliyun底层主机使用的是什么虚拟化技术，但是根据相关文档还是找到了，丢失的网卡id，并进行挂载。
## 问题总结
### 阿里云Terway网络工作模式研究 
